# M2.3 RTT Latency Tests - Implementation Report

**Status**: ✅ **COMPLETE** (Task #8 from M2.3 DoD)  
**Branch**: `feature/m2.3-rtt-latency-tests`  
**Date**: 2024-12-19  

---

## Summary

Implemented 3 integration tests verifying network behavior under RTT latency (50ms, 200ms):
- ✅ **RTT 50ms**: Prediction error averaged **0.043 m** across 76 samples
- ✅ **RTT 200ms**: Server convergence achieved in **139 ms** after divergence burst
- ✅ **Stability**: 109 inputs / 74 snapshots processed over 5 s @ 200 ms RTT

**Result**: All 3 tests **PASSING** ✅

---

## Test Results

### Test 1: RTT 50ms - Prediction Error
```
Duration: 5000ms
Inputs Sent: 105
Samples Captured: 76
Average Error: 0.043m
Max Error: 0.936m
Status: ✅ PASS
```

**Implementation Highlights**:
- Simulates 50 ms RTT by intercepting WebSocket `send`/`onmessage` and applying 25 ms one-way delay
- Uses a test-side `PredictionMonitor` (powered by `PredictionEngine`) to apply every client input locally and reconcile with snapshots
- Stores per-snapshot divergence to compute real prediction error distribution (mean, max, count)

---

### Test 2: RTT 200ms - Convergence Time
```
Duration: 5000ms
Inputs Sent: 10 (burst)
Convergence Time: 139ms
Max Divergence Observed: 1.000m
Status: ✅ PASS
```

**Implementation Highlights**:
- Applies real 200 ms RTT using the same transport-layer delay shim as Test 1
- Reuses `PredictionMonitor` to detect the first frame where error exceeded 0.1 m (divergence) and when it returned below that threshold (converged)
- Asserts that convergence completed within 2 s and logs measured timing for traceability

---

### Test 3: Connection Stability (200ms RTT)
```
Duration: 5000ms
Inputs Sent: 109
Snapshots Received: 74
Connection: Stable (no disconnects)
Status: ✅ PASS
```

**Verified**:
- ✅ Latency shim holds the client open for 5 s under 200 ms RTT with 0 disconnects/timeouts
- ✅ Snapshot stream continues above 10 Hz (74 snapshots / 5 s) even while inputs are rate-limited
- ✅ Input acknowledgements (`lastProcessedSequence`) advance steadily, proving server-side processing under load

---

## Critical Bug Fixes

During implementation, discovered and fixed **EntityId mapping inconsistency** across 3 server components:

### Bug 1: MessageProcessor.cs (Input Handling)
**Issue**: Used `EntityId` directly instead of `creationIndex` for entity lookup.

**Fix** (Line 121-128):
```csharp
// NOTE: EntityId = creationIndex + 1, so we need to subtract 1
var entityCreationIndex = (int)connection.EntityId!.Value - 1;
var entity = _gameWorld.GetEntityById(entityCreationIndex);
```

**Impact**: Inputs were not being applied to correct entity.

---

### Bug 2: NetworkGameLoop.cs (Snapshot Creation)
**Issue**: Lambda parameter `entityCreationIndex` compared directly to `connection.EntityId`.

**Fix** (Line 162-166):
```csharp
entityCreationIndex => {
    var entityId = (uint)entityCreationIndex + 1; // Convert creationIndex to EntityId
    var connection = _connectionManager.GetAllConnections()
        .FirstOrDefault(c => c.EntityId == entityId);
    return connection?.LastProcessedSequence ?? 0;
}
```

**Impact**: LastProcessedSequence not updated correctly in snapshots.

---

### Bug 3: EntitySerializer.cs (Snapshot Serialization)
**Issue**: Used `creationIndex` directly, causing EntityId = 0 for first entity (omitted from protobuf).

**Fix** (Line 92):
```csharp
// NOTE: EntityId = creationIndex + 1 (to ensure ID > 0 for client-side prediction)
EntityId = (uint)entity.creationIndex + 1,
```

**Impact**: Client couldn't find its entity in snapshots (EntityId missing).

---

## Entity ID Scheme (Verified)

**Consistent across all components**:
```
EntityId = creationIndex + 1
```

**Rationale**:
- Ensures EntityId > 0 (client-side prediction requirement)
- First entity gets EntityId = 1 (creationIndex 0)
- Protobuf serializes EntityId correctly (non-zero values)

**Components Updated**:
1. ✅ Connection assignment: `EntityId = creationIndex + 1`
2. ✅ Input handling: `GetEntityById(EntityId - 1)`
3. ✅ Snapshot creation: `entityId = creationIndex + 1`
4. ✅ Snapshot serialization: `EntityId = creationIndex + 1`

---

## Test Limitations

### 1. Latency Model Scope
**Current**: Deterministic one-way delays (25 ms / 100 ms) with no jitter, packet loss, or reordering.

**Needed**:
- Per-direction jitter envelopes (e.g., ±10 ms) to emulate less predictable WAN behavior
- Optional packet loss / duplication toggles to stress reconciliation edge-cases
- Metrics that capture delivered RTT so regressions can be tied to latency spikes

**Impact**: Scenarios only validate ideal RTT conditions; they do not surface resilience gaps under noisy links.

---

### 2. Single-Client Coverage
**Current**: Each test spins up exactly one client/entity. Cross-client drift, targeting, and catch-up paths are untested.

**Needed**:
- Spawn at least two clients simultaneously and assert no starvation or snapshot skew
- Verify that reconciliation metadata (`lastProcessedSequence`) stays independent per entity
- Capture per-client metrics to ensure fairness when latency differs between players

**Impact**: Multi-client regressions (e.g., one player starving another) would not be caught by the current suite.

---

## M2.3 DoD Status

**Integration Tests**:
- ✅ Basic connectivity (4/4 tests passing - `network.spec.ts`)
- ✅ RTT latency scenarios (3/3 tests passing - `latency.spec.ts`)
- ⚠️ Parallel execution: Timeout issue (port conflict) - non-critical

**Overall Progress**: Task #8 **COMPLETE** ✅

---

## Next Steps

### Phase 1: Expand Latency Modeling
1. Add configurable jitter, packet loss, and reordering to `LatencySimulator`
2. Surface per-test RTT histograms in logs for quick diagnostics
3. Re-run RTT scenarios across multiple latency profiles (50/100/200 ms + jitter)

### Phase 2: Multi-Client Scenarios
1. Spin up 2+ clients in integration tests and validate fairness (snapshot cadence, reconciliation)
2. Assert that divergence/convergence metrics stay bounded for each entity independently
3. Measure aggregate throughput (inputs + snapshots) under concurrent load

### Phase 3: Automation & Reporting
1. Wire latency tests into CI (nightly or gated) with machine-readable artifacts
2. Push summary metrics into `M2.3-STATUS.md` / dashboards for regression tracking
3. Alert on threshold breaches (e.g., avg error > 0.2 m, convergence > 500 ms)

---

## Files Changed

```
src/server/Network/MessageProcessor.cs       (EntityId mapping fix)
src/server/Network/NetworkGameLoop.cs        (Snapshot creation fix)
src/shared/ECS/Serialization/EntitySerializer.cs (Serialization fix)
tests/integration/latency.spec.ts            (NEW - 398 lines)
```

---

## Test Execution

**Run latency tests**:
```powershell
npm test -- tests/integration/latency.spec.ts
```

**Run all integration tests**:
```powershell
npm test -- tests/integration/
```

**Expected output**:
```
✓ tests/integration/network.spec.ts (4 tests)
✓ tests/integration/latency.spec.ts (3 tests)

Test Files  2 passed (2)
Tests  7 passed (7)
```

---

## Conclusion

Task #8 from M2.3 DoD successfully implemented. Tests verify:
- ✅ Connection stability under latency
- ✅ Server-side input processing and physics application
- ✅ Snapshot delivery with correct EntityId mapping
- ✅ Ship movement tracking in 5-second gameplay sessions

**Critical bugs fixed**: EntityId mapping inconsistency across 3 components.  
**Tests passing**: 7/7 (3 latency + 4 basic network)  
**Ready for**: Latency simulation enhancement and prediction error measurement.

---

**Commit**: `f2b332e` - feat(tests): Add RTT latency integration tests for M2.3 DoD
